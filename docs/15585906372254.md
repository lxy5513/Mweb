# RGBD 

RGB image + depth image 

<p align="center"><img src="https://raw.githubusercontent.com/lxy5513/Markdown_image_dateset/master/Xnip2019-05-23_13-55-57.jpg" width="70%" alt="" /></p>

## 深度值和三维数据
深度图像可以看做是一副灰度图像，其中图像中每个点的灰度值代表了这个点的深度值，即该点在现实中的位置到相机所在垂直平面的真实距离 
<p align="center"><img src="https://raw.githubusercontent.com/lxy5513/Markdown_image_dateset/master/Xnip2019-05-23_14-11-04.jpg" width="70%" alt="" /></p>

对于现实场景中**点M**，深度相机能够获取其在RGB图像中的==成像点XM==，以及M到==相机所在的垂直平面==（即XY平面）的距离，这个距离便是M的深度值。以相机位置为原点，相机所朝方向为Z轴，相机的垂直平面的两个轴向为X、Y轴，可以建立相机的局部三维坐标系。另外，RGB图像到相机位置的距离正是相机的焦距。通过这些数据并使用简单的三角几何公式，我们很容易得到M在相机的局部坐标系中的三维坐标。于是，RGB图像中的每个点，都会对应一个在相机的局部坐标系中的三维点。因此，深度相机的每一帧的深度图像就相当于一个在相机的局部三维坐标系中的点云模型。


## 局部三维坐标系与全局三维坐标系 
每一帧深度图像对应的点云模型是在相机的局部三维坐标系中。因此，不同的相机位置（即不同帧）便对应着不同的局部三维坐标系（local space/coordinate frame）。然而，重建后的模型需要坐落在一个坐标系，即世界坐标系或全局坐标系（world/global space/coordinate frame）中。于是，我们需要找到每一帧的相机局部坐标系同世界坐标系的位置关系，也就是确定每一帧中相机在世界坐标系中的位置
<p align="center"><img src="https://raw.githubusercontent.com/lxy5513/Markdown_image_dateset/master/Xnip2019-05-23_14-23-10.jpg" width="70%" alt="" /></p>



## RGBD和RGB + D
我们强调RGBD和RGB + D术语之间的区别。 当两个模态按像素对齐时，我们建议使用RGBD;当两个模式的分辨率不同且帧不对齐时，我们建议使用RGB + D