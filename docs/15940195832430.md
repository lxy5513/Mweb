# engine inference 

simplebaseline
```
with open("pose_dynamic.engine", "rb") as f, trt.Runtime(TRT_LOGGER) as runtime:
    self.engine = runtime.deserialize_cuda_engine(f.read())
inputs, outputs, bindings, stream = allocate_buffers(self.engine, batch_size)
context = self.engine.create_execution_context()
# 动态输入的关键
context.set_binding_shape(0, (batch_size, 192, 192, 3))
np.copyto(inputs[0].host, imgs.ravel())
[cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]
# Run inference.
context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)  # tensorrt 7+ dynamic batch
# Transfer predictions back from the GPU.
[cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]
# Synchronize the stream
stream.synchronize()
results = [out.host for out in outputs]
```

