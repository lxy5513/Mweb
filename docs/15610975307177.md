## 指定GPU
```
export CUDA_VISIBLE_DEVICES="1" 
# 这个数量要和number_of_gpu匹配。

在config里面， 设置：
number_of_gpu=1
```

## 限制GPU的数量
```
Limit GPU usage
By default, MMS will use all available GPUs for inference, you use number_of_gpu to limit the usage of GPU
```

## Https
For users who want to enable HTTPs, you can change inference_address or management_addrss protocol from http to https


## default_workers_per_model
default_workers_per_model: number of workers to create for each model that loaded at startup time, default: available GPUs in system or number of logical processors available to the JVM. 





## 部署pytorch model指定GPU：1 

os.environ["CUDA_VISIBLE_DEVICES"]="0"
self.device = "cuda"