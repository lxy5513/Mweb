#  并行训练计算 

一般的单GPU
```

predictions = model(inputs)               # 前向计算
loss = loss_function(predictions, labels) # 计算损失函数
loss.backward()                           # 后向计算梯度
optimizer.step()                          # 优化器更新梯度
predictions = model(inputs)               # 用更新过的参数值进行下一次前向计算
```

多GPU 

```

parallel_model = torch.nn.DataParallel(model) # 就是这里！
 
predictions = parallel_model(inputs)          # 前向计算
loss = loss_function(predictions, labels)     # 计算损失函数
loss.mean().backward()                        # 计算多个GPU的损失函数平均值，计算梯度
optimizer.step()                              # 反向传播
predictions = parallel_model(inputs)
```