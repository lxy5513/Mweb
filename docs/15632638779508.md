# Load and predict with ONNX Runtime 

This example demonstrates how to load a model and compute the output for an input vector. It also shows how to retrieve the definition of its inputs and outputs.
```
import onnxruntime
sess = onnxruntime.InferenceSession("st-gcn.onnx")

input_name = sess.get_inputs()[0].name
print("input name", input_name)
input_shape = sess.get_inputs()[0].shape
print("input shape", input_shape)
input_type = sess.get_inputs()[0].type
print("input type", input_type)

output_name = sess.get_outputs()[0].name
print("output name", output_name)
output_shape = sess.get_outputs()[0].shape
print("output shape", output_shape)
output_type = sess.get_outputs()[0].type
print("output type", output_type)

# inference
import numpy.random
x = numpy.random.random((1,3,300,18,2))
x = x.astype(numpy.float32)
res = sess.run([output_name], {input_name: x})
```