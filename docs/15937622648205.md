#  Tensorrt 流程

一、训练好模型（无关tensorRT)

二、 Developing A Deployment Solution

 	1. 根据神经网络的功能（应用场景）来创造和验证一种部署方案，部署架构。
 	2. 用tensorrt建造一个inference engine
      	1. 根据神经网络存储的形式，用解析器(ONNX parser, Caffe parser, TF parser)将其转换为TensorRT
	3. 在网络被解析后，考虑优化选项
    	1. batch size, workspace size, mixed percision, bounds on dynamic shape 发生在tensorrt build过程中
	4. 验证tensorrt复现的结果，此时可以选择FP32, FP16, INT8
	5. 写出这个inference engine in a serialized format, 称为plan file ( Plans are specific to the exact GPU model they were built on)

三、 Deploying A Solution

 	1. TensorRT 库 will be linked to the 部署应用。
 	2. deserialize the plan model 到 inference engine
 	3. 异步调用tensorRT, 程序通过数据输入队列来得到input buffer