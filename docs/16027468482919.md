# 注意力机制

注意力机制（Attention Mechanism）是机器学习中的一种数据处理方法，广泛应用在自然语言处理、图像识别及语音识别等各种不同类型的机器学习任务中。注意力机制本质上与人类对外界事物的观察机制相似。通常来说，人们在观察外界事物的时候，**首先**会比较关注比较倾向于观察事物某些重要的局部信息，**然后**再把不同区域的信息组合起来，**从而形成**一个对被观察事物的整体印象

Attention Mechanism可以帮助模型**对输入的X每个部分赋予不同的权重**，抽取出更加关键及重要的信息，使模型做出更加准确的判断，同时不会对模型的计算和存储带来更大的开销，这也是Attention Mechanism应用如此广泛的原因。

---


#### 软注意力(soft attention)与强注意力(hard attention)的不同之处在于：

软注意力更关注区域或者通道，而且软注意力是==确定性==的注意力，学习完成后直接可以通过网络生成，最关键的地方是软注意力是**可微**的，这是一个非常重要的地方。可以微分的注意力就可以通过神经网络算出梯度并且前向传播和后向反馈来学习得到注意力的权重。 在计算机视觉中，很多领域的相关工作(例如，分类、检测、分割、生成模型、视频处理等)都在使用Soft Attention，典型代表：SENet、SKNet。
强注意力是更加关注==点==，也就是图像中的每个点都有可能延伸出注意力，同时强注意力是一个随机的预测过程，更==强调动态变化==。当然，最关键是强注意力是一个**不可微**的注意力，训练过程往往是通过增强学习(reinforcement learning) 来完成的。

---

## CV中的注意力机制

近几年来，深度学习与视觉注意力机制结合的研究工作，大多数是集中于使用**掩码(mask)**来形成注意力机制。掩码的原理在于通过另一层新的权重，将图片数据中关键的特征标识出来，通过学习训练，让深度神经网络学到每一张新图片中需要关注的区域，也就形成了注意力。

计算机视觉中的注意力机制的基本思想是让模型学会专注，把注意力集中在重要的信息上而忽视不重要的信息。

Attention机制的本质就是==利用相关特征图学习权重分布==，再用学出来的权重施加在原特征图之上最后进行加权求和。不过**施加权重**的方式略有差别，大致总结为如下四点：

- 保留所有分量均做加权（即soft attention）；也可以是在分布中以某种采样策略选取部分分量（即hard attention），此时常用RL来做。
- 作用在空间尺度上，给不同空间区域加权；
- 作用在Channel尺度上，给不同通道特征加权；
- 作用在不同时刻历史特征上，结合循环结构添加权重，例如机器翻译，或者视频相关的工作。

为了更清楚地介绍计算机视觉中的注意力机制，通常将注意力机制中的模型结构分为三大注意力域来分析。主要是：空间域(spatial domain)，通道域(channel domain)，混合域(mixed domain)。

空间域——将图片中的的空间域信息做对应的==空间变换==，从而能将关键的信息提取出来。对空间进行掩码的生成，进行**打分**，代表是Spatial Attention Module。

通道域——类似于给每个通道上的信号都增加一个权重，来代表该通道与关键信息的相关度的话，这个权重越大，则表示相关度越高。对通道生成掩码mask，进行打分，代表是senet, Channel Attention Module。

混合域——空间域的注意力是忽略了通道域中的信息，将每个通道中的图片特征同等处理，这种做法会将空间域变换方法局限在原始图片特征提取阶段，应用在神经网络层其他层的可解释性不强。

---

## 卷积神经网络中常用的Attention

在卷积神经网络中常用到的主要有两种：一种是spatial attention, 另外一种是channel attention。当然有时也有使用空间与通道混合的注意力，其中混合注意力的代表主要是BAM, CBAM。

###Spatial Attention：
对于卷积神经网络，CNN每一层都会输出一个C x H x W的特征图，C就是通道，同时也代表卷积核的数量，亦为特征的数量，H 和W就是原始图片经过压缩后的图的高度和宽度，

Spatial Attention就是对于所有的通道，在二维平面上，对H x W尺寸的特征图学习到一个权重，对每个像素都会学习到一个权重。你可以想象成一个像素是C维的一个向量，深度是C，在C个维度上，权重都是一样的，但是在平面上，权重不一样。

###Channel Attention
对于每个C（通道），在channel维度上，学习到不同的权重，平面维度上权重相同。所以基于通道域的注意力通常是对一个通道内的信息直接全局平均池化，而忽略每一个通道内的局部信息。

spatial 和 channel attention可以理解为关注图片的不同区域和关注图片的不同特征。channel attention的全面介绍可以参考论文：SCA-CNN，通道注意力在图像分类中的网络结构方面，典型的就是SENet。




---


下文将主要介绍：注意力机制在分类网络中的典型应用--SENet

视觉注意力机制在分类网络中的应用