# Learning Efﬁcient Convolutional Networks through Network Slimming 

## Abstract 
在许多实际的应用中，深度卷积神经网络很大程度上被其高昂的计算代价所局限。在本文中，我们提出一个新颖的方法处理CNN，使其能同时：1. 减少模型的大小。2. 减少其运行时的内存占用。3. 在不影响准确性的情况下降低计算操作的数量。 
这是通过以简单但有效的方式在网络中实施信道级（channel0=-level)稀疏性来实现的.此方法是在训练过程中引入最小的开销，并且不需要特殊的软件、硬件加速器来生成模型。
它将大型网络作为输入模型，在训练过程中，没有什么价值的channeld被自动定义，然后为别修建。我们实验表明在最新的几种网络结构：VGG ResNet DenseNet, 在不同的图片分类数据集上都会有很好的效果。

